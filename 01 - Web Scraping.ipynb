{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a combination of BeautifulSoup and Selenium to fetch data. <br>\n",
    "We then save the data to .csv files for later use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Packages <a name=\"import\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium\n",
    "from selenium import webdriver\n",
    "import chromedriver_binary\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents <a name=\"table\"></a>\n",
    "1. [Import Packages](#import)\n",
    "2. [Scraping Data from...](#scrape)\n",
    "    1. [U.S. News and World Report](#usnews)\n",
    "    2. [Nature](#nature)\n",
    "    3. [Google Scholar](#google)\n",
    "3. [Scraping Methods Assemble](#assemble)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Scraping Data from... <a name=\"scrape\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### U.S. News & World Report <a name=\"usnews\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroller(driver, num_scrolls):\n",
    "    \"\"\"\n",
    "        Helper function to scroll down pages that need\n",
    "        time to render due to infinite pagination\n",
    "        \n",
    "        :param driver: Selenium driver for scrolling down\n",
    "        :param num_scrolls: the number of times to scroll down\n",
    "    \"\"\"\n",
    "    \n",
    "    scroll_pause_time = 1\n",
    "\n",
    "    scroll_ct = 0\n",
    "\n",
    "    while scroll_ct < num_scrolls:\n",
    "\n",
    "        # Get scroll height\n",
    "        #This is the difference. Moving this *inside* the loop\n",
    "        # means that it checks if scrollTo is still scrolling \n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "\n",
    "            # try again (can be removed)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            # Wait to load page\n",
    "            time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "            # Calculate new scroll height and compare with last scroll height\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            # check if the page height has remained the same\n",
    "            if new_height == last_height:\n",
    "                # if so, you are done\n",
    "                break\n",
    "            # if not, move on to the next loop\n",
    "            else:\n",
    "                last_height = new_height\n",
    "                continue\n",
    "                \n",
    "        scroll_ct+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rankings():\n",
    "    \"\"\"\n",
    "        Function to get best biological school ranking info\n",
    "        from U.S. News & Worlds Report\n",
    "        \n",
    "        :returns: uni_df, pandas Dataframe of rankings for schools\n",
    "    \"\"\"\n",
    "    \n",
    "    rankings_url = 'https://www.usnews.com/best-graduate-schools/\\\n",
    "                    top-science-schools/biological-sciences-rankings'\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(rankings_url)\n",
    "    scroller(driver, 10)\n",
    "    rankings_soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "    uni_list = rankings_soup.find_all('div', {'class':'Box-s85n6m5-0 kKdFhD'})\n",
    "\n",
    "    uni_dict = {}\n",
    "    for idx, uni in enumerate(uni_list):\n",
    "        if idx == 100: #only want 100 schools\n",
    "            break\n",
    "\n",
    "        name = uni.get('name')\n",
    "        name = name.replace('--', ', ') #change '--' in name like 'University of California -- Berkeley' to\n",
    "                                        #'University of California, Berkeley'\n",
    "\n",
    "        rank = (uni.find('strong', {'class':'s144f3me-0-Strong-kDSDFS eULIZs'}).contents) #get tag with rank\n",
    "        rank = ''.join(rank) #join elements in list to make string including rank\n",
    "        rank = re.findall('\\d+',rank)[0] #find the number\n",
    "        uni_dict[name] = int(rank)\n",
    "        \n",
    "    uni_df = pd.DataFrame.from_dict({'name':list(uni_dict.keys()), \n",
    "                                     'rank':list(uni_dict.values())})\n",
    "    uni_df = (uni_df.sort_values(by=['rank'])\n",
    "                    .reset_index()\n",
    "                    .drop(columns = ['index']))\n",
    "    \n",
    "    return uni_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_uni_ranking(uni_list, uni_df):\n",
    "    \"\"\"\n",
    "        Function that checks how many universities are in \n",
    "        the top 20, top 50, or top 100 of the list \n",
    "        from U.S. News and Worlds Report\n",
    "        \n",
    "        :param uni_list: list of universities to check\n",
    "        :param uni_df: list of ranking from U.S. News and Worlds Report\n",
    "\n",
    "        :returns articles: list of Nature article hrefs for that year\n",
    "    \"\"\"\n",
    "    seen_uni = set()\n",
    "    top20, top50, top100, other = 0, 0, 0, 0\n",
    "    for uni in uni_list:\n",
    "        if uni in seen_uni:\n",
    "            continue\n",
    "        rank_cnt = uni_df[uni_df['name'].str.contains(uni)]['rank'].sum()\n",
    "        if rank_cnt != 0:\n",
    "            for ranking in uni_df[uni_df['name'].str.contains(uni)]['rank']:\n",
    "                ranking = int(ranking)\n",
    "                if ranking <= 100:\n",
    "                    top100 = 1\n",
    "                if ranking <= 50:\n",
    "                    top50 = 1\n",
    "                if ranking <= 20:\n",
    "                    top20 = 1\n",
    "            seen_uni.add(uni)\n",
    "        else:\n",
    "            other = 1\n",
    "                \n",
    "    return top20, top50, top100, other"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2B. Nature <a name=\"nature\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nature_links(year):\n",
    "     \"\"\"\n",
    "        Function to fetch links to all the articles written\n",
    "        for Nature for a specified year\n",
    "        \n",
    "        :param year: year during which Nature has articles for e.g. 2019\n",
    "        \n",
    "        :returns articles: a list of Nature article hrefs for that year\n",
    "    \"\"\"\n",
    "    \n",
    "    nature_url = 'http://www.nature.com/nature/articles?searchType\\\n",
    "                    =journalSearch&sort=PubDate&type=article&year=' \n",
    "    nature_url = nature_url + str(year)\n",
    "   \n",
    "    articles = []\n",
    "    \n",
    "    nature_response  = requests.get(nature_url)\n",
    "    nature_page = nature_response.text\n",
    "    nature_soup = BeautifulSoup(nature_page, 'lxml')\n",
    "    \n",
    "    #find the number of pages there are to go through for this specific year in Nature\n",
    "    num_pages = len(set(page.get('href') for page in \n",
    "                        nature_soup.select('a[href^=\"/nature/articles?\"]'))) + 1\n",
    "                    \n",
    "    nature_url = nature_url + '&page='\n",
    "                    \n",
    "    while page_cnt < num_pages + 1: \n",
    "        nature_response  = requests.get(nature_url + str(page_cnt))\n",
    "        nature_page = nature_response.text\n",
    "        nature_soup = BeautifulSoup(nature_page, 'lxml')\n",
    "        entries = nature_soup.find_all('a', {'data-track-action':'view article'})\n",
    "        for entry in entries:\n",
    "            articles.append(entry.get('href'))\n",
    "        page_cnt += 1\n",
    "                                       \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def article_info(href):\n",
    "    \"\"\"\n",
    "        Function to scrape information from a Nature article\n",
    "        from a specific href\n",
    "        \n",
    "        :param href: a href link to a specific Nature article\n",
    "        \n",
    "        :returns title_length: title length for Nature article\n",
    "        :returns altmetric: almetric score for Nature article\n",
    "        :returns num_times_cited: number of times Nature article was cited\n",
    "        :returns abstract_length: numbers of words in article abstract\n",
    "        :returns page_length: number of pages for Nature article\n",
    "        :returns fig_count: number of figures in Nature article\n",
    "        :returns ref_cnt: number of cited references in Nature article\n",
    "        :returns authors: list of authors who wrote Nature article\n",
    "        :returns uni_list: list of institutions authors came from\n",
    "        :returns num_institutions: number of institutions authors came from\n",
    "    \"\"\"\n",
    "   \n",
    "    nature_url = 'https://www.nature.com' + href\n",
    "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "    \n",
    "    try:\n",
    "        nature_response  = requests.get(nature_url, headers = user_agent) \n",
    "        time.sleep(0.5+2*random.random())\n",
    "    except:\n",
    "        time.sleep(60)\n",
    "        nature_response  = requests.get(nature_url, headers = user_agent)\n",
    "    \n",
    "    #find the title and the number of words in the title\n",
    "    nature_page = nature_response.text\n",
    "    nature_soup = BeautifulSoup(nature_page, 'lxml')\n",
    "    title = nature_soup.find('meta', {'name':'citation_title'}).get('content')\n",
    "    title_length = len(title.split(' '))\n",
    "    \n",
    "    #find the abstract and the length of the abstract\n",
    "    abstract = nature_soup.find('meta', {'name':'dc.description'})\n",
    "    abstract = abstract.get('content').split(' ')\n",
    "    abstract_length = len(abstract)\n",
    "    \n",
    "    #find the page length\n",
    "    pageStart = int(nature_soup.find('span', {'itemprop':'pageStart'}).text)\n",
    "    pageEnd = int(nature_soup.find('span', {'itemprop':'pageEnd'}).text)\n",
    "    page_length = pageEnd - pageStart\n",
    "    \n",
    "    #find the number of figures in an article\n",
    "    fig_items = nature_soup.find_all('div', class_=\"c-article-section__figure-item\")\n",
    "    fig_count = len(fig_items) \n",
    "\n",
    "    if fig_count == 0:\n",
    "        try:\n",
    "            fig_items = nature_soup.find_all('img', {'data-component':'rc-content-image'})\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    #find the number of references in an article\n",
    "    references = nature_soup.find_all('p', {'class':'c-article-references__text'})\n",
    "    ref_cnt = len(references)\n",
    "    \n",
    "    if ref_cnt == 0:\n",
    "        try:\n",
    "            #sometimes you have to use a differet \n",
    "            references = nature_soup.find_all('cite')\n",
    "            ref_cnt = len(references)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    try:\n",
    "        num_times_cited = nature_soup.find('p', {'data-test':'citation-count'}).text.split(' ')[0]\n",
    "        altmetric = nature_soup.find('p', {'data-test':'altmetric-score'}).text.split(' ')[0]\n",
    "    except:\n",
    "        metrics = nature_soup.find_all('p', {'class':'c-article-metrics-bar__count'})\n",
    "        num_times_cited = metrics[0].contents[0]\n",
    "        altmetric = metrics[1].contents[0] #metric used to help measure impact of paper \n",
    "    \n",
    "    authors = [author.get('content') for author in nature_soup.find_all('meta', {'name':'dc.creator'})]\n",
    "\n",
    "    \n",
    "    uni_addr_list = nature_soup.find_all('h4', {'class': 'c-article-author-affiliation__address'})\n",
    "    \n",
    "    \n",
    "    if len(uni_addr_list) == 0:\n",
    "        uni_addr_list = nature_soup.find_all('h3', {'class':'emphasis'})\n",
    "    \n",
    "    uni_list = []\n",
    "    num_institutions = 0\n",
    "\n",
    "    for uni_addr in uni_addr_list:\n",
    "        uni = uni_addr.text.split(', ')[:2]\n",
    "        num_institutions += 1\n",
    "        try:\n",
    "            #some addresses are not clear, but the name of the institution\n",
    "            #is usually included in either of the first two values, \n",
    "            #so we add both values\n",
    "            for i in range(0, 2):             \n",
    "                uni_list.append(uni[i]) \n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "    return (title_length, altmetric, num_times_cited, \n",
    "            abstract_length, page_length, fig_count, \n",
    "            ref_cnt, authors, uni_list, num_institutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2C. Google Scholar <a name=\"google\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def author_info(name):\n",
    "    \"\"\"\n",
    "        Function to search up an author on Google Scholar \n",
    "        and find info on them from their profile\n",
    "        \n",
    "        :param href: a href link to a specific Nature article\n",
    "        \n",
    "        :returns citations: numbers of citations\n",
    "        :returns h_index: impact factor h_index of author\n",
    "        :returns num_keywords: number of keyword tags author uses\n",
    "    \"\"\"\n",
    "    \n",
    "    google_url = 'https://scholar.google.com'\n",
    "    scholar_url = google_url + '/citations?view_op=search_\\\n",
    "                                    authors&hl=en&mauthors=' + str(name)\n",
    "    \n",
    "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "    scholar_response  = requests.get(scholar_url, headers = user_agent)\n",
    "\n",
    "    time.sleep(4+2*random.random())\n",
    "\n",
    "    scholar_page = scholar_response.text\n",
    "    scholar_soup = BeautifulSoup(scholar_page, 'lxml')\n",
    "        \n",
    "    href = (scholar_soup.find('h3', {'class':'gs_ai_name'}) #find the parent tag\n",
    "                        .findChild('a') #find the child tag that has the href\n",
    "                        .get('href')) #get the href\n",
    "    \n",
    "    profile_url = google_url + href\n",
    "    profile_response  = requests.get(profile_url)\n",
    "    profile_page = profile_response.text\n",
    "    profile_soup = BeautifulSoup(profile_page, 'lxml')\n",
    "\n",
    "    metrics = profile_soup.find_all('td', {'class':\"gsc_rsb_std\"})[::2]\n",
    "    citations = int(metrics[0].text)\n",
    "    h_index = int(metrics[1].text)\n",
    "    i10_index = int(metrics[2].text)\n",
    "\n",
    "    num_keywords = len(profile_soup.find_all('a', {'class': 'gsc_prf_inta gs_ibl'}))\n",
    "    \n",
    "    return citations, h_index, i10_index, num_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [Table of Contents](#table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scraping Methods Assemble <a name=\"assemble\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have written all the scraping functions, we can actually begin running our functions to start collecting the data for our linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Massachusetts Institute of Technology</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Stanford University</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>University of California, Berkeley</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>California Institute of Technology</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Harvard University</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    name  rank\n",
       "0  Massachusetts Institute of Technology     1\n",
       "1                    Stanford University     1\n",
       "2     University of California, Berkeley     1\n",
       "3     California Institute of Technology     4\n",
       "4                     Harvard University     4"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only need to run the follow two lines of code once \n",
    "#comment out once done\n",
    "if 'data' not in os.listdir():\n",
    "    os.mkdir('data')\n",
    "    \n",
    "file_dir = os.path.abspath('.')\n",
    "csv_folder = 'data'\n",
    "path = os.path.join(file_dir, csv_folder, year + 'university_rankings.csv')\n",
    "\n",
    "rankings = get_rankings()\n",
    "rankings.to_csv(path)\n",
    "\n",
    "rankings = pd.read_csv(path)\n",
    "rankings = rankings.drop(columns = 'Unnamed: 0')\n",
    "uni_df = rankings\n",
    "uni_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_info(href):\n",
    "    \"\"\"\n",
    "        Function to scrape info from Nature and Google Scholar\n",
    "        \n",
    "        :param href: article link to scrape data from \n",
    "        \n",
    "        :returns df: one-row pandas dataframe containing scraped results\n",
    "    \"\"\"\n",
    "\n",
    "    info = article_info(href)     \n",
    "\n",
    "    authors = info[-3]\n",
    "    num_authors = len(authors)\n",
    "\n",
    "    if len(authors) > 5:\n",
    "        authors = authors[:5]\n",
    "\n",
    "    total_citations = []\n",
    "    total_h_index = []\n",
    "    total_il0_index = []\n",
    "\n",
    "    for author in authors:\n",
    "        try: \n",
    "            author_metrics = author_info(author)\n",
    "            citations = author_metrics[0]\n",
    "            total_citations.append(citations)\n",
    "            h_index = author_metrics[1]\n",
    "            total_h_index.append(h_index)\n",
    "            i10_index = author_metrics[2]\n",
    "            total_il0_index.append(i10_index)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    if len(total_citations) != 0:\n",
    "        mean_author_citations = round(sum(total_citations)/len(total_citations), 2)\n",
    "        mean_h_index = round(sum(total_h_index)/len(total_h_index), 2)\n",
    "        mean_il0_index = round(sum(total_il0_index)/len(total_il0_index), 2)\n",
    "    else:\n",
    "        mean_author_citations = np.nan\n",
    "        mean_h_index = np.nan\n",
    "        mean_il0_index = np.nan\n",
    "\n",
    "    uni_list = info[-2]\n",
    "    try:\n",
    "        top20, top50, top100, other = check_uni_ranking(uni_list, uni_df)\n",
    "    except:\n",
    "        top20, top50, top100, other = np.nan, np.nan, np.nan, 1\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['title_length']= [info[0]]\n",
    "    df['altmetric'] = info[1]\n",
    "    df['num_times_cited'] = info[2]\n",
    "    df['abstract_length'] = info[3]\n",
    "    df['page_length'] = info[4]\n",
    "    df['fig_count'] = info[5]\n",
    "    df['ref_cnt'] = info[-4]\n",
    "    df['mean_author_citations'] = mean_author_citations\n",
    "    df['mean_h_index'] = mean_h_index\n",
    "    df['mean_i10_index'] = mean_il0_index\n",
    "    df['num_authors'] = num_authors\n",
    "    df['top20'] = top20\n",
    "    df['top50'] = top50\n",
    "    df['top100'] = top100\n",
    "    df['other'] = other\n",
    "    df['num_insitutions'] = info[-1]\n",
    "    df['year'] = year\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each year given below, we want to fetch data and save it to a .csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [str(2009+i) for i in range(0, 10)]\n",
    "\n",
    "if 'data' not in os.listdir():\n",
    "    os.mkdir('data')\n",
    "    \n",
    "file_dir = os.path.abspath('.')\n",
    "csv_folder = 'data'\n",
    "    \n",
    "for year in years:\n",
    "    article_df = pd.DataFrame()\n",
    "    hrefs = generate_nature_links(year)\n",
    "    for href in hrefs:\n",
    "        df = collect_info()\n",
    "        article_df = pd.concat([article_df, df])\n",
    "        \n",
    "    path = os.path.join(file_dir, csv_folder, year + '_article_metrics.csv')\n",
    "    article_df.to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Return to [Table of Contents](#table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
